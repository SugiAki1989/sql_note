## :memo: Overview

今回はさくらのクラウドのGPUを使ってLLMモデルを動かすところまで実行してみる。さくらインターネットさんのサービスを使うのは、昔にVPSを借りていたとき以来になる。なので、さくらのクラウド自体も初めて。

下記の記事を参考にして、サーバーの起動からモデル実行までを行う。

- [さくらのクラウドのGPUサーバ(Tesla V100)でサイバーエージェントが一般公開した日本語LLM(OpenCALM)を動かしてみる](https://qiita.com/tar_xzvf/items/09ee2bf146c4a3319492#6%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%82%92%E5%8B%95%E3%81%8B%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8Bopen-calm-7b)

## :floppy_disk: Database

null

## :bookmark: Tag

sakura internet

## :pencil2: Sample

さくらのクラウドからサーバーを設定して起動する。最新のubuntuを使っていると、Step2のところで、エラーが出るので注意。ubuntu24はNVIDIAのCUDAリポジトリはまだ対応していない可能性が高いので、ubuntu22を使うほうが無難。

![server](https://github.com/SugiAki1989/sql_note/blob/main/image/p149-1.png)

サーバーが設定できれば、画像の通り、管理画面からIPアドレスを取得して、sshでログイン。あとは下記の通り実行すればOK。

```
# Step1: sshで立ち上げたGPUサーバーにアクセス
$ ssh ubuntu@************

# Step2: GPUドライバのインストール
$ sudo apt-get install linux-headers-$(uname -r)
$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\.//g')
$ wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-keyring_1.0-1_all.deb
$ sudo dpkg -i cuda-keyring_1.0-1_all.deb
$ sudo apt-get update
$ sudo apt-get -y install cuda-drivers

# Step3: pythonライブラリのインストール
$ sudo apt install python3-pip
$ pip3 install transformers accelerate
$ python3

# Step4: サンプルコードを動かす(open-calm-7b)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-7b", device_map="auto", torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-7b")

def generate_text(prompt, model, tokenizer, max_new_tokens=64, temperature=0.7, top_p=0.9, repetition_penalty=1.05):
    """
    指定したプロンプトに基づいてテキストを生成する関数。

    Parameters:
        prompt (str): 入力テキスト（プロンプト）
        model (transformers.PreTrainedModel): テキスト生成モデル
        tokenizer (transformers.PreTrainedTokenizer): モデルに対応するトークナイザ
        max_new_tokens (int): 生成する最大トークン数
        temperature (float): サンプリング温度
        top_p (float): nucleus sampling の確率閾値
        repetition_penalty (float): 生成文の繰り返し抑制
    Returns:
        str: 生成されたテキスト
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        tokens = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            pad_token_id=tokenizer.pad_token_id,
        )
    return tokenizer.decode(tokens[0], skip_special_tokens=True)

prompt = '「さくらのクラウド」によって私達の暮らしは、'
generate_text(prompt, model, tokenizer)
```
![output](https://github.com/SugiAki1989/sql_note/blob/main/image/p149-2.png)

1時間遊んでみて、1000円くらいでした。
誤ってubuntu24を入れて、ubuntu22に変更したので、本来は500円くらいで済んだと思われる。
料金体系への理解も深まったので良しとする。

![price](https://github.com/SugiAki1989/sql_note/blob/main/image/p149-3.png)


### おわりに

初めてさくらのクラウドを使ったが、上記の作業は10分くらいで完了できた、数時間使ってみて感想は下記の通り。

- 国産なので説明や表記がわかりやすい。海外クラウドの日本語？みたいなことがなさそう。
- サーバーの設定画面でも料金計算が表示されるのはすごくいい。クラウド破産怖いし。
- 為替を考えなくていいのは料金の把握が楽。
- サーバーの起動、削除とかもAWSのEC2とかに比べて、すごく速く感じた。

## :closed_book: Reference

- [さくらのクラウドのGPUサーバ(Tesla V100)でサイバーエージェントが一般公開した日本語LLM(OpenCALM)を動かしてみる](https://qiita.com/tar_xzvf/items/09ee2bf146c4a3319492#6%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%82%92%E5%8B%95%E3%81%8B%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8Bopen-calm-7b)
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) 